{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefit of feature selection in practice\n",
    "\n",
    "### Speed-up train and scoring time\n",
    "The principal advantage of selecting features within a machine learning\n",
    "pipeline is to reduce the time to train this pipeline and its time to\n",
    "predict. We will give an example to highlights these advantages. First, we\n",
    "generate a synthetic dataset to control the number of features that will be\n",
    "informative, redundant, repeated, and random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=5000,\n",
    "    n_features=100,\n",
    "    n_informative=2,\n",
    "    n_redundant=0,\n",
    "    n_repeated=0,\n",
    "    random_state=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose to create a dataset with two informative features among a hundred.\n",
    "To simplify our example, we did not include either redundant or repeated\n",
    "features.\n",
    "\n",
    "We will create two machine learning pipelines. The former will be a random\n",
    "forest that will use all available features. The latter will also be a random\n",
    "forest, but we will add a feature selection step to train this classifier.\n",
    "The feature selection is based on a univariate test (ANOVA F-value) between\n",
    "each feature and the target that we want to predict. The features with the\n",
    "two most significant scores are selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model_without_selection = RandomForestClassifier(n_jobs=-1)\n",
    "model_with_selection = make_pipeline(\n",
    "    SelectKBest(score_func=f_classif, k=2),\n",
    "    RandomForestClassifier(n_jobs=-1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will measure the average time spent to train each pipeline and make it\n",
    "predict. Besides, we will compute the generalization score of the model. We\n",
    "will collect these results via cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cv_results_without_selection = pd.DataFrame(\n",
    "    cross_validate(model_without_selection, X, y)\n",
    ")\n",
    "cv_results_with_selection = pd.DataFrame(\n",
    "    cross_validate(model_with_selection, X, y, return_estimator=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.concat(\n",
    "    [cv_results_without_selection, cv_results_with_selection],\n",
    "    axis=1,\n",
    "    keys=[\"Without feature selection\", \"With feature selection\"],\n",
    ").swaplevel(axis=\"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first analyze the train and score time for each pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cv_results[\"fit_time\"].plot.box(vert=False, whis=100)\n",
    "plt.xlabel(\"Elapsed time (s)\")\n",
    "_ = plt.title(\"Time to fit the model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results[\"score_time\"].plot.box(vert=False, whis=100)\n",
    "plt.xlabel(\"Elapsed time (s)\")\n",
    "_ = plt.title(\"Time to make prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can draw the same conclusions for both training and scoring elapsed time:\n",
    "selecting the most informative features speed-up our pipeline.\n",
    "\n",
    "Of course, such speed-up is beneficial only if the performance in terms of\n",
    "metrics remain the same. Let's check the generalization score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results[\"test_score\"].plot.box(vert=False, whis=100)\n",
    "plt.xlabel(\"Accuracy score\")\n",
    "_ = plt.title(\"Test score via cross-validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the model's performance selecting a subset of features\n",
    "decreases compared with the model using all available features. Since we\n",
    "generated the dataset, we can infer that the decrease is because the\n",
    "selection did not choose the two informative features.\n",
    "\n",
    "We can quickly investigate which feature have been selected during the\n",
    "cross-validation. We will print the indices of the two selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for idx, pipeline in enumerate(cv_results_with_selection[\"estimator\"]):\n",
    "    print(\n",
    "        f\"Fold #{idx} - features selected are: \"\n",
    "        f\"{np.argsort(pipeline[0].scores_)[-2:]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the feature `53` is always selected while the other feature\n",
    "varies depending on the cross-validation fold.\n",
    "\n",
    "If we would like to keep our score with similar performance, we could choose\n",
    "another metric to perform the test or select more features. For instance, we\n",
    "could select the number of features based on a specific percentile of the\n",
    "highest scores. Besides, we should keep in mind that we simplify our problem\n",
    "by having informative and not informative features. Correlation between\n",
    "features makes the problem of feature selection even harder.\n",
    "\n",
    "Therefore, we could come with a much more complicated procedure that could\n",
    "fine-tune (via cross-validation) the number of selected features and change\n",
    "the way feature is selected (e.g. using a machine-learning model). However,\n",
    "going towards these solutions alienates the feature selection's primary\n",
    "purpose to get a significant train/test speed-up. Also, if the primary goal\n",
    "was to get a more performant model, performant models exclude non-informative\n",
    "features natively.\n",
    "\n",
    "## Caveats of the feature selection\n",
    "When using feature selection, one has to be extra careful about the way it\n",
    "implements it. We will show two examples where feature selection can\n",
    "miserably fail.\n",
    "\n",
    "### Selecting features without cross-validation\n",
    "The biggest mistake to be made when selecting features is similar to one that\n",
    "can be made when optimizing hyperparameters of a model: find the subset of\n",
    "features on the same dataset as well used to evaluate the model's\n",
    "generalization performance.\n",
    "\n",
    "We will generate a synthetic dataset with a large number of features and a\n",
    "few samples to emphasize the issue. This use-case is typical in\n",
    "bioinformatics when dealing with RNA-seq. However, we will use completely\n",
    "randomized features such that we don't have a link between the data and the\n",
    "target. Thus, the performance of any machine-learning model should not\n",
    "perform better than the chance-level. In our example, we will use a logistic\n",
    "regressin classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "X, y = rng.randn(100, 100000), rng.randint(0, 2, size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "test_score = cross_val_score(model, X, y, n_jobs=-1)\n",
    "print(f\"The mean accuracy is: {test_score.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no surprise that the logistic regression model performs as the\n",
    "chance level when we provide the full dataset.\n",
    "\n",
    "We will then show the **wrong** pattern that one should not apply: select the\n",
    "feature by using the entire dataset. We will choose ten features with the\n",
    "highest ANOVA F-score computed on the full dataset. Subsequently, we\n",
    "subsample the dataset `X` by selecting the features' subset. Finally, we\n",
    "train and test a logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "feature_selector = SelectKBest(score_func=f_classif, k=10)\n",
    "test_score = cross_val_score(model, feature_selector.fit_transform(X, y), y)\n",
    "print(f\"The mean accuracy is: {test_score.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, the logistic regression succeeded in having a fantastic\n",
    "accuracy using data with no link with the target, initially. We, therefore,\n",
    "know that these results are not legit.\n",
    "\n",
    "The reasons for obtaining these results are two folds: the pool of available\n",
    "features is large compared to the number of samples. It is possible to find a\n",
    "subset of features that will link the data and the target. By not splitting\n",
    "the data, we leak knowledge from the entire dataset and could use this\n",
    "knowledge will evaluating our model.\n",
    "\n",
    "Instead, we will now split our dataset into a training and testing set and\n",
    "only compute the univariate test on the training set. Then, we will use the\n",
    "best features found on the training set during the scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(feature_selector, LogisticRegression())\n",
    "test_score = cross_val_score(model, X, y)\n",
    "print(f\"The mean accuracy is: {test_score.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that selecting feature only on the training set will not help when\n",
    "testing our model. In this case, we obtained the expected results.\n",
    "\n",
    "Therefore, as with hyperparameters optimization or model selection, tuning\n",
    "the feature space should be done solely on the training set, keeping a part\n",
    "of the data left-out.\n",
    "\n",
    "### Limitation of selecting feature using a model\n",
    "An advanced strategy to select features is to use a machine learning model.\n",
    "Indeed, one can inspect a model and find relative feature importances. For\n",
    "instance, the parameters `coef_` for the linear models or\n",
    "`feature_importances_` for the tree-based models carries such information.\n",
    "Therefore, this method works as far as the relative feature importances given\n",
    "by the model is sufficient to select the meaningful feature.\n",
    "\n",
    "Here, we will generate a dataset that contains a large number of random\n",
    "features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(\n",
    "    n_samples=5000,\n",
    "    n_features=100,\n",
    "    n_informative=2,\n",
    "    n_redundant=5,\n",
    "    n_repeated=5,\n",
    "    class_sep=0.3,\n",
    "    random_state=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's build a model which will not make any features selection. We\n",
    "will use a cross-validation to evaluate this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_without_selection = RandomForestClassifier(n_jobs=-1)\n",
    "cv_results_without_selection = pd.DataFrame(\n",
    "    cross_validate(model_without_selection, X, y, cv=5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will build another model which will include a feature selection\n",
    "step based on a random forest. We will also evaluate the performance of the\n",
    "model via cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "model_with_selection = make_pipeline(\n",
    "    SelectFromModel(\n",
    "        estimator=RandomForestClassifier(n_jobs=-1),\n",
    "    ),\n",
    "    RandomForestClassifier(n_jobs=-1),\n",
    ")\n",
    "cv_results_with_selection = pd.DataFrame(\n",
    "    cross_validate(model_with_selection, X, y, cv=5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the generalization score of the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.concat(\n",
    "    [cv_results_without_selection, cv_results_with_selection],\n",
    "    axis=1,\n",
    "    keys=[\"Without feature selection\", \"With feature selection\"],\n",
    ").swaplevel(axis=\"columns\")\n",
    "cv_results[\"test_score\"].plot.box(vert=False, whis=100)\n",
    "plt.xlabel(\"Accuracy\")\n",
    "_ = plt.title(\"Limitation of using a random forest for feature selection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model that selected a subset of feature is less performant than a\n",
    "random forest fitted on the full dataset.\n",
    "\n",
    "We can rely on some aspects tackled in the notebook presenting the model\n",
    "inspection to explain this behaviour. The decision tree's relative feature\n",
    "importance will overestimate the importance of random feature when the\n",
    "decision tree overfits the training set.\n",
    "\n",
    "Therefore, it is good to keep in mind that feature selection relies on\n",
    "procedures making some assumptions, which can be perfectible."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
