{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of your predictive model: the cross-validation framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In the previous notebooks, we check how to fit a machine-learning model. When\n",
    "we evaluate our model's performance, we did not detail the evaluation\n",
    "framework that one should use in machine-learning. This notebook presents the\n",
    "cross-validation framework and emphasizes the importance of evaluating a\n",
    "model with such a framework.\n",
    "\n",
    "Besides, we will show some good practices to follow, such as nested\n",
    "cross-validation, when tuning model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test datasets\n",
    "Before discussing the cross-validation framework, we will linger on the\n",
    "reasons for always having training and testing sets. Let's first look at the\n",
    "limitation of using a unique dataset.\n",
    "\n",
    "### Load the California housing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "X, y = housing.data, housing.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this dataset to predict the median value of houses in an area in\n",
    "California. The feature collected are based on general real-estate\n",
    "and geographical information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(housing.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify future visualization, we transform the target in k\\$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y *= 100\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empirical error vs generalization error\n",
    "\n",
    "As mentioned previously, we start by fitting a decision tree regressor on the\n",
    "full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "regressor = DecisionTreeRegressor()\n",
    "regressor.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the regressor, we would like to know the regressor's potential\n",
    "performance once deployed in production. For this purpose, we use the mean\n",
    "absolute error, which gives us an error in the native unit, i.e. k\\$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y_pred = regressor.predict(X)\n",
    "score = mean_absolute_error(y_pred, y)\n",
    "print(f\"In average, our regressor make an error of {score:.2f} k$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A perfect prediction is with no error is too optimistic and almost always\n",
    "revealing a methodological problem when doing machine learning.\n",
    "\n",
    "Indeed, we trained and predicted on the same dataset. Since our decision tree\n",
    "was fully grown, every sample in the dataset is stored in a leaf node.\n",
    "Therefore, our decision tree fully memorized the dataset given during `fit`\n",
    "and make no single error when predicting on the same data.\n",
    "\n",
    "This error computed above is called the **empirical error** or **training\n",
    "error**.\n",
    "\n",
    "We trained a predictive model to minimize the empirical error but our aim is\n",
    "to minimize the error on data that has not been seen during training.\n",
    "\n",
    "This error is also called the **generalization error** or the \"true\"\n",
    "**testing error**. Thus, the most basic evaluation involves:\n",
    "\n",
    "* splitting our dataset into two subsets: a training set and a testing set;\n",
    "* fitting the model on the training set;\n",
    "* estimating the empirical error on the training set;\n",
    "* estimating the generalization error on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_train)\n",
    "score = mean_absolute_error(y_pred, y_train)\n",
    "print(f\"The empirical error of our model is {score:.2f} k$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test)\n",
    "score = mean_absolute_error(y_pred, y_test)\n",
    "print(f\"The generalization error of our model is {score:.2f} k$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stability of the cross-validation estimates\n",
    "\n",
    "When doing a single train-test split we don't give any indication\n",
    "regarding the robustness of the evaluation of our predictive model: in\n",
    "particular, if the test set is small, this estimate of the generalization\n",
    "error can be unstable and do not reflect the \"true error rate\" we would have\n",
    "observed with the same model on an unlimitted amount of test data.\n",
    "\n",
    "For instance, we could have been lucky when we did our random split of our\n",
    "limited dataset and isolated some of the easiest cases to predict in the\n",
    "testing set just by chance: the estimation of the generalization error would\n",
    "be overly optimistic, in this case.\n",
    "\n",
    "**Cross-validation** allows estimating the robustness of a predictive model\n",
    "by repeating the splitting procedure. It will give several empirical and\n",
    "generalization errors and thus some **estimate of the variability of the\n",
    "model performance**.\n",
    "\n",
    "There are different cross-validation strategies, for now we are going to\n",
    "focus on one called \"shuffle-split\". At each iteration of this strategy we:\n",
    "\n",
    "- shuffle the order of the samples of a copy of the full data at random;\n",
    "- split the shuffled dataset into a train and a test set;\n",
    "- train a new model on the train set;\n",
    "- evaluate the generalization error on the test set.\n",
    "\n",
    "We repeat this procedure `n_splits` times. Using `n_splits=30` means that we\n",
    "will train 30 models in total and all of them will be discarded: we just\n",
    "record their performance on each variant of the test set.\n",
    "\n",
    "To evaluate the performance of our regressor, we can use `cross_validate`\n",
    "with a `ShuffleSplit` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "cv = ShuffleSplit(n_splits=30, test_size=0.2)\n",
    "\n",
    "cv_results = cross_validate(\n",
    "    regressor, X, y, cv=cv, scoring=\"neg_mean_absolute_error\",\n",
    ")\n",
    "cv_results = pd.DataFrame(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By convention, scikit-learn model evaluation tools always use a convention\n",
    "where \"higher is better\", this explains we used\n",
    "`scoring=\"neg_mean_absolute_error\"` (meaning \"negative mean absolute error\").\n",
    "\n",
    "Let us revert the negation to get the actual error and not the negative\n",
    "score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results[\"test_error\"] = -cv_results[\"test_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the results reported by the cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get timing information to fit and predict at each round of\n",
    "cross-validation. Also, we get the test score, which corresponds to the\n",
    "generalization error on each of the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get 30 entries in our resulting dataframe because we performed 30 splits.\n",
    "Therefore, we can show the generalization error distribution and thus, have\n",
    "an estimate of its variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.displot(cv_results[\"test_error\"], kde=True, bins=10)\n",
    "_ = plt.xlabel(\"Mean absolute error (k$)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the generalization error is clustered around 45.5 k\\\\$ and\n",
    "ranges from 43 k\\\\$ to 49 k\\\\$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"The mean cross-validated generalization error is: \"\n",
    "    f\"{cv_results['test_error'].mean():.2f} k$\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"The standard deviation of the generalization error is: \"\n",
    "    f\"{cv_results['test_error'].std():.2f} k$\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the standard deviation is much smaller than the mean:\n",
    "we could summarize that our cross-validation estimate of the\n",
    "generalization error is 45.7 +/- 1.1 k\\\\$.\n",
    "\n",
    "If we were to train a single model on the full dataset (without\n",
    "cross-validation) and then had later access to an unlimited\n",
    "amount of test data, we would expect its true generalization\n",
    "error to fall close to that region.\n",
    "\n",
    "While this information is interesting in it-self, this should be\n",
    "contrasted to the scale of the natural variability of the target `y`\n",
    "in our dataset.\n",
    "\n",
    "Let us plot the distribution of the target variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(y, kde=True, bins=20)\n",
    "_ = plt.xlabel(\"Median House Value (k$)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The standard deviation of the target is: {y.std():.2f} k$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable ranges from close to 0 k\\\\$ up to 500 k\\\\$ and, with a\n",
    "standard deviation around 115 k\\\\$.\n",
    "\n",
    "We notice that the mean estimate of the generalization error obtained\n",
    "by cross-validation is a bit than the natural scale of variation\n",
    "of the target variable. Furthermore the standard deviation of the cross\n",
    "validation estimate of the generalization error is even much smaller.\n",
    "\n",
    "This is a good start, but not necessarily enough to decide whether the\n",
    "generalization performance is good enough to make our prediction useful in\n",
    "practice.\n",
    "\n",
    "We recall that our model makes, on average, an error around 45 k\\$. With this\n",
    "information and looking at the target distribution, such an error might be\n",
    "acceptable when predicting houses with a 500 k\\\\$. However, it would be an\n",
    "issue with a house with a value of 50 k\\\\$. Thus, this indicates that our\n",
    "metric (Mean Absolute Error) is not ideal.\n",
    "\n",
    "We might instead choose a metric relative to the target\n",
    "value to predict: the mean absolute percentage error would have been a much\n",
    "better choice.\n",
    "\n",
    "But in all cases, an error of 45 k\\$ might be too large to automatically use\n",
    "our model to tag house value without expert supervision.\n",
    "\n",
    "To better understand the performance of our model and maybe find insights\n",
    "on how to improve it we will compare the generalization\n",
    "error with the empirical error. Thus, we need to compute the error on the\n",
    "training set, which is possible using the `cross_validate` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = cross_validate(\n",
    "    regressor, X, y, cv=cv, scoring=\"neg_mean_absolute_error\",\n",
    "    return_train_score=True,\n",
    "    n_jobs=2\n",
    ")\n",
    "cv_results = pd.DataFrame(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame()\n",
    "scores[[\"train_error\", \"test_score\"]] = -cv_results[[\"train_score\", \"test_score\"]]\n",
    "sns.histplot(scores, bins=50)\n",
    "_ = plt.xlabel(\"Mean absolute error (k$)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "By plotting the distribution of the empirical and generalization errors, we\n",
    "get information about whether our model is over-fitting, under-fitting (or\n",
    "both at the same time).\n",
    "\n",
    "Here, we observe a **small empirical error** (actually zero), meaning that\n",
    "the model is **not under-fitting**: it is flexible enough to capture any\n",
    "variations present in the training set.\n",
    "\n",
    "However the **significantly larger generalization error** tells us that the\n",
    "model is **over-fitting**: the model has memorized many variations of the\n",
    "training set that could be considered \"noisy\" because they do not generalize\n",
    "to help us make good prediction on the test set.\n",
    "\n",
    "Some model hyper-parameters are usually the key to go from a model that\n",
    "underfits to a model that overfits, hopefully going through a region were we\n",
    "can get a good balance between the two. We can acquire knowledge by plotting\n",
    "a curve called the validation curve. This curve applies the above experiment\n",
    "and varies the value of a hyper-parameter.\n",
    "\n",
    "For the decision tree, the `max_depth` the main parameter to control the\n",
    "trade-off between under-fitting and over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "max_depth = [1, 5, 10, 15, 20, 25]\n",
    "train_scores, test_scores = validation_curve(\n",
    "    regressor, X, y,\n",
    "    param_name=\"max_depth\", param_range=max_depth,\n",
    "    cv=cv, scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "ax.plot(\n",
    "    max_depth, -train_scores.mean(axis=1),\n",
    "    linestyle=\"-.\", label=\"empirical error\",\n",
    "    alpha=0.8,\n",
    ")\n",
    "ax.fill_between(\n",
    "    max_depth,\n",
    "    -train_scores.mean(axis=1) - train_scores.std(axis=1),\n",
    "    -train_scores.mean(axis=1) + train_scores.std(axis=1),\n",
    "    alpha=0.5,\n",
    ")\n",
    "ax.plot(\n",
    "    max_depth, -test_scores.mean(axis=1),\n",
    "    linestyle=\"-.\", label=\"Generalization error\",\n",
    "    alpha=0.8,\n",
    ")\n",
    "ax.fill_between(\n",
    "    max_depth,\n",
    "    -test_scores.mean(axis=1) - test_scores.std(axis=1),\n",
    "    -test_scores.mean(axis=1) + test_scores.std(axis=1),\n",
    "    alpha=0.5,\n",
    ")\n",
    "\n",
    "ax.set_xticks(max_depth)\n",
    "ax.set_xlabel(\"Maximum depth of decision tree\")\n",
    "ax.set_ylabel(\"Mean absolute error (k$)\")\n",
    "ax.set_title(\"Validation curve for decision tree\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation curve can be divided into three areas:\n",
    "\n",
    "- For `max_depth < 10`, the decision tree underfits. The empirical error and\n",
    "  therefore also the generalization error are both high. The model is too\n",
    "  constrained and cannot capture much of the varibility of the target\n",
    "  variable.\n",
    "\n",
    "\n",
    "- The region around `max_depth = 10` corresponds to the parameter for which\n",
    "  the decision tree generalizes the best. It is flexible enough to capture a\n",
    "  fraction of the variability of the target that generalizes, while not\n",
    "  memorizing all of the noise in the target.\n",
    "\n",
    "\n",
    "- For `max_depth > 10`, the decision tree overfits. The empirical error\n",
    "  becomes very small, while the generalization error increases. In this\n",
    "  region, the models captures too much of the noisy part of the variations of\n",
    "  the target and this harms its ability to generalize well to test data.\n",
    "\n",
    "\n",
    "Note that for `max_depth = 10`, the model overfits a bit as there is a gap\n",
    "between the empirical error and the generalization error. It can also\n",
    "potentially underfit also a bit at the same time, because the empirical error\n",
    "is still far from zero (more than 30 k\\\\$), meaning that the model might\n",
    "still be too constrained to model interesting parts of the data. However the\n",
    "generalization error is minimal, and this is what really matters. This is the\n",
    "best compromise we could reach by just tuning this parameter.\n",
    "\n",
    "We were lucky that the variance of the errors was small compared to their\n",
    "respective values, and therefore the conclusions above are quite clear. This\n",
    "is not necessarily always the case.\n",
    "\n",
    "We will now focus on one factor that can affect this variance, namely, the\n",
    "size of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Let's do an experiment and reduce the number of samples and repeat the\n",
    "previous experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def make_cv_analysis(regressor, X, y):\n",
    "    cv = ShuffleSplit(n_splits=10, test_size=0.2)\n",
    "    cv_results = pd.DataFrame(\n",
    "        cross_validate(\n",
    "            regressor, X, y, cv=cv, scoring=\"neg_mean_absolute_error\",\n",
    "            return_train_score=True\n",
    "        )\n",
    "    )\n",
    "    return (cv_results[\"test_score\"] * -1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sample_sizes = [100, 500, 1000, 5000, 10000, 15000, y.size]\n",
    "\n",
    "scores_sample_sizes = {\"# samples\": [], \"test score\": []}\n",
    "rng = np.random.RandomState(0)\n",
    "for n_samples in sample_sizes:\n",
    "    sample_idx = rng.choice(np.arange(y.size), size=n_samples, replace=False)\n",
    "    X_sampled, y_sampled = X.iloc[sample_idx], y[sample_idx]\n",
    "    score = make_cv_analysis(regressor, X_sampled, y_sampled)\n",
    "    scores_sample_sizes[\"# samples\"].append(n_samples)\n",
    "    scores_sample_sizes[\"test score\"].append(score)\n",
    "\n",
    "scores_sample_sizes = pd.DataFrame(\n",
    "    np.array(scores_sample_sizes[\"test score\"]).T,\n",
    "    columns=scores_sample_sizes[\"# samples\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(scores_sample_sizes, kind=\"kde\")\n",
    "plt.xlabel(\"Mean absolute error (k$)\")\n",
    "_ = plt.title(\n",
    "    \"Generalization errors distribution \\nby varying the sample size\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the different sample sizes, we plotted the distribution of the\n",
    "generalization error. We observe that smaller is the sample size; larger is\n",
    "the variance of the generalization errors. Thus, having a small number of\n",
    "samples might put us in a situation where it is impossible to get a reliable\n",
    "evaluation.\n",
    "\n",
    "Here, we plotted the different curve to highlight the issue of small sample\n",
    "size. However, this experiment is also used to draw the so-called **learning\n",
    "curve**. This curve gives some additional indication regarding the benefit of\n",
    "adding new training samples to improve a model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "results = learning_curve(\n",
    "    regressor,\n",
    "    X,\n",
    "    y,\n",
    "    train_sizes=sample_sizes[:-1],\n",
    "    cv=cv,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=2,\n",
    ")\n",
    "train_size, train_scores, test_scores = results[:3]\n",
    "train_errors, test_errors = -train_scores, -test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "ax.plot(\n",
    "    train_size,\n",
    "    train_errors.mean(axis=1),\n",
    "    linestyle=\"-.\",\n",
    "    label=\"empirical error\",\n",
    "    alpha=0.8,\n",
    ")\n",
    "ax.fill_between(\n",
    "    train_size,\n",
    "    train_errors.mean(axis=1) - train_errors.std(axis=1),\n",
    "    train_errors.mean(axis=1) + train_errors.std(axis=1),\n",
    "    alpha=0.5,\n",
    ")\n",
    "ax.plot(\n",
    "    train_size,\n",
    "    test_errors.mean(axis=1),\n",
    "    linestyle=\"-.\",\n",
    "    label=\"Generalization error\",\n",
    "    alpha=0.8,\n",
    ")\n",
    "ax.fill_between(\n",
    "    train_size,\n",
    "    test_errors.mean(axis=1) - test_errors.std(axis=1),\n",
    "    test_errors.mean(axis=1) + test_errors.std(axis=1),\n",
    "    alpha=0.5,\n",
    ")\n",
    "\n",
    "ax.set_xticks(train_size)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(\"Number of samples in the training set\")\n",
    "ax.set_ylabel(\"Mean absolute error (k$)\")\n",
    "ax.set_title(\"Learning curve for decision tree\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the more samples we add to the training set on this learning\n",
    "curve, the lower the error becomes. With this curve, we are searching for the\n",
    "plateau for which there is no benefit to adding samples anymore or assessing\n",
    "the potential gain of adding more samples into the training set.\n",
    "\n",
    "For this dataset we notice that our decision tree model would really benefit\n",
    "from additional datapoints to reduce the amount of over-fitting and hopefully\n",
    "reduce the generalization error even further.\n",
    "\n",
    "\n",
    "## Comparing results with baseline and chance level\n",
    "\n",
    "Previously, we compare the generalization error by taking into account the\n",
    "target distribution. A good practice is to compare the generalization error\n",
    "with a dummy baseline and the chance level. In regression, we could use the\n",
    "`DummyRegressor` and predict the mean target without using the data. The\n",
    "chance level can be determined by permuting the labels and check the\n",
    "difference of result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "dummy = DummyRegressor()\n",
    "result_dummy = cross_validate(\n",
    "    dummy,\n",
    "    X,\n",
    "    y,\n",
    "    cv=cv,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import permutation_test_score\n",
    "\n",
    "score, permutation_score, pvalue = permutation_test_score(\n",
    "    regressor,\n",
    "    X,\n",
    "    y,\n",
    "    cv=cv,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=-1,\n",
    "    n_permutations=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the generalization errors for each of the experiments. Even if our\n",
    "regressor does not perform well, it is far above a regressor that would\n",
    "predict the mean target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = pd.concat(\n",
    "    [\n",
    "        cv_results[\"test_score\"] * -1,\n",
    "        pd.Series(result_dummy[\"test_score\"]) * -1,\n",
    "        pd.Series(permutation_score) * -1,\n",
    "    ],\n",
    "    axis=1,\n",
    ").rename(\n",
    "    columns={\n",
    "        \"test_score\": \"Cross-validation score\",\n",
    "        0: \"Dummy score\",\n",
    "        1: \"Permuted score\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(final_result, kind=\"kde\")\n",
    "_ = plt.xlabel(\"Mean absolute error (k$)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice of cross-validation\n",
    "In the previous section, we presented the cross-validation framework.\n",
    "However, we always use the `ShuffleSplit` strategy to repeat the split. One\n",
    "should question if this approach is always the best option and that some\n",
    "other cross-validation strategies would be better adapted. Indeed, we will\n",
    "focus on three aspects that influenced the choice of the cross-validation\n",
    "strategy: class stratification, sample grouping, feature dependence.\n",
    "\n",
    "### Stratification\n",
    "Let's start with the concept of stratification by giving an example where\n",
    "we can get into trouble if we are not careful. We load the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "X, y = load_iris(as_frame=True, return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we create a basic machine-learning model: a logistic\n",
    "regression. We expect this model to work quite well on the iris dataset since\n",
    "this is a toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(\n",
    "    StandardScaler(), LogisticRegression(max_iter=1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we created our model, we will use the cross-validation framework to\n",
    "evaluate it. We will use a strategy called `KFold` cross-validation. We give\n",
    "a simple usage example of the `KFold` strategy to get an intuition of how it\n",
    "splits the dataset. We will define a dataset with nine samples and repeat the\n",
    "cross-validation three times (i.e. `n_splits`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X_random = np.random.randn(9, 1)\n",
    "cv = KFold(n_splits=3)\n",
    "for train_index, test_index in cv.split(X_random):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By defining three splits, we will use three samples each time for testing and\n",
    "6 for training. `KFold` does not shuffle by default. It means that it will\n",
    "select the three first samples for the testing set at the first split, then\n",
    "the three next three samples for the second split, and the three next for the\n",
    "last split. In the end, all samples have been used in testing at least once\n",
    "among the different splits.\n",
    "\n",
    "Now, let's apply this strategy to check the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=3)\n",
    "results = cross_validate(model, X, y, cv=cv)\n",
    "test_score = results[\"test_score\"]\n",
    "print(f\"The average accuracy is {test_score.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a real surprise that our model cannot correctly classify any sample in\n",
    "any cross-validation split. We will now check our target's value to\n",
    "understand the issue while we should have started with this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at our target, samples of a class are grouped together. Also, the\n",
    "sample count per class is the same. Thus, splitting the data with three\n",
    "splits use all samples of a single class during testing. So our model is\n",
    "unable to predict this class that was unseen during the training stage.\n",
    "\n",
    "One possibility to solve the issue is to shuffle the data before to split the\n",
    "data into three groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=3, shuffle=True, random_state=0)\n",
    "results = cross_validate(model, X, y, cv=cv)\n",
    "test_score = results[\"test_score\"]\n",
    "print(f\"The average accuracy is {test_score.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get results that are closer to what we would expect with an accuracy above\n",
    "90%. Now that we solved our first issue, it would be interesting to check if\n",
    "the class frequency in the training and testing set is equal to our original\n",
    "set's class frequency. It would ensure that we are training and testing our\n",
    "model with a class distribution that we will encounter in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in cv.split(X, y):\n",
    "    print(\n",
    "        f\"Class frequency in the training set:\\n\"\n",
    "        f\"{y[train_index].value_counts(normalize=True).sort_index()}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Class frequency in the testing set:\\n\"\n",
    "        f\"{y[test_index].value_counts(normalize=True).sort_index()}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that neither the training and testing sets have the same class\n",
    "frequencies as our original dataset. Thus, it means that one might want to\n",
    "split our data by preserving the original class frequencies: we want to\n",
    "**stratify** our data by class. In scikit-learn, some cross-validation\n",
    "strategies are implementing the stratification and contains `Stratified` in\n",
    "their names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3)\n",
    "for train_index, test_index in cv.split(X, y):\n",
    "    print(\n",
    "        f\"Class frequency in the training set:\\n\"\n",
    "        f\"{y[train_index].value_counts(normalize=True).sort_index()}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Class frequency in the testing set:\\n\"\n",
    "        f\"{y[test_index].value_counts(normalize=True).sort_index()}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_validate(model, X, y, cv=cv)\n",
    "test_score = results[\"test_score\"]\n",
    "print(f\"The average accuracy is {test_score.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we observe that the class frequencies are very close. The\n",
    "difference is due to the small number of samples in the iris dataset.\n",
    "\n",
    "In conclusion, this is a good practice to use stratification within the\n",
    "cross-validation framework when dealing with a classification problem.\n",
    "\n",
    "## Sample grouping\n",
    "We are going to linger into the concept of samples group. As in the previous\n",
    "section, we will give an example to highlight some surprising results. This\n",
    "time, we will use the handwritten digits dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same baseline model. We will use a `KFold` cross-validation\n",
    "without shuffling the data at first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(shuffle=False)\n",
    "results = cross_validate(model, X, y, cv=cv, n_jobs=-1)\n",
    "test_score_no_shuffling = results[\"test_score\"]\n",
    "print(f\"The average accuracy is {test_score_no_shuffling.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(shuffle=True)\n",
    "results = cross_validate(model, X, y, cv=cv, n_jobs=-1)\n",
    "test_score_with_shuffling = results[\"test_score\"]\n",
    "print(f\"The average accuracy is {test_score_with_shuffling.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that shuffling the data allows to improving the mean accuracy.\n",
    "We could go a little further and plot the distribution of the generalization\n",
    "score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=pd.DataFrame(\n",
    "    [test_score_no_shuffling, test_score_with_shuffling],\n",
    "    index=[\"KFold without shuffling\", \"KFold with shuffling\"],\n",
    ").T, kde=True, bins=10)\n",
    "plt.xlim([0.8, 1.0])\n",
    "plt.xlabel(\"Accuracy score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-validation generalization error that uses the shuffling has less\n",
    "variance than the one that does not impose any shuffling. It means that some\n",
    "specific fold leads to a low score in the unshuffle case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_score_no_shuffling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, there is an underlying structure in the data that shuffling will break\n",
    "and get better results. To get a better understanding, we should read the\n",
    "documentation shipped with the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(digits.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we read carefully, 13 writers wrote the digits of our dataset, accounting\n",
    "for a total amount of 1797 samples. Thus, a writer wrote several times the\n",
    "same numbers. Let's suppose that the writer samples are grouped.\n",
    "Subsequently, not shuffling the data will keep all writer samples together\n",
    "either in the training or the testing sets. Mixing the data will break this\n",
    "structure, and therefore digits written by the same writer will be available\n",
    "in both the training and testing sets.\n",
    "\n",
    "Besides, a writer will usually tend to write digits in the same manner. Thus,\n",
    "our model will learn to identify a writer's pattern for each digit instead of\n",
    "recognizing the digit itself.\n",
    "\n",
    "We can solve this problem by ensuring that the data associated with a writer\n",
    "should either belong to the training or the testing set. Thus, we want to\n",
    "group samples for each writer.\n",
    "\n",
    "Here, we will manually define the group for the 13 writers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "\n",
    "# defines the lower and upper bounds of sample indices\n",
    "# for each writer\n",
    "writer_boundaries = [\n",
    "    0, 130, 256, 386, 516, 646, 776, 915, 1029,\n",
    "    1157, 1287, 1415, 1545, 1667, 1797\n",
    "]\n",
    "groups = np.zeros_like(y)\n",
    "\n",
    "for group_id, lower_bound, upper_bound in zip(\n",
    "    count(),\n",
    "    writer_boundaries[:-1],\n",
    "    writer_boundaries[1:]\n",
    "):\n",
    "    groups[lower_bound:upper_bound] = group_id\n",
    "groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we group the digits by writer, we can use cross-validation to take this\n",
    "information into account: the class containing `Group` should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "cv = GroupKFold()\n",
    "results = cross_validate(model, X, y, groups=groups, cv=cv, n_jobs=-1)\n",
    "test_score = results[\"test_score\"]\n",
    "print(f\"The average accuracy is {test_score.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this strategy is less optimistic regarding the model performance.\n",
    "However, this is the most reliable if our goal is to make handwritten digits\n",
    "recognition writers independent.\n",
    "\n",
    "## Non i.i.d. data\n",
    "In machine learning, it is quite common to assume that the data are i.i.d,\n",
    "meaning that the generative process does not have any memory of past samples\n",
    "to generate new samples.\n",
    "\n",
    "This assumption is usually violated when dealing with time series. A sample\n",
    "depends on past information.\n",
    "\n",
    "We will take an example to highlight such issues with non-i.i.d. data in the\n",
    "previous cross-validation strategies presented. We are going to load\n",
    "financial quotations from some energy companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = {\n",
    "    \"TOT\": \"Total\",\n",
    "    \"XOM\": \"Exxon\",\n",
    "    \"CVX\": \"Chevron\",\n",
    "    \"COP\": \"ConocoPhillips\",\n",
    "    \"VLO\": \"Valero Energy\",\n",
    "}\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/scikit-learn/examples-data/\"\n",
    "    \"master/financial-data/{}.csv\"\n",
    ")\n",
    "\n",
    "quotes = {}\n",
    "for symbol in symbols:\n",
    "    data = pd.read_csv(url.format(symbol), index_col=0, parse_dates=True)\n",
    "    quotes[symbols[symbol]] = data[\"open\"]\n",
    "quotes = pd.DataFrame(quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start by plotting different financial quotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(10, 6))\n",
    "_ = quotes.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can formulate the following regression problem. We want to be able to\n",
    "predict the quotation of Chevron using all other energy companies' quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = quotes.drop(columns=[\"Chevron\"]), quotes[\"Chevron\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a decision tree regressor that we expect to overfit and thus not\n",
    "generalize to unseen data. We will use a `ShuffleSplit` cross-validation to\n",
    "check the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "regressor = DecisionTreeRegressor()\n",
    "\n",
    "cv = ShuffleSplit(random_state=0)\n",
    "results = cross_validate(regressor, X_train, y_train, cv=cv, n_jobs=-1)\n",
    "test_score = results[\"test_score\"]\n",
    "print(f\"The mean R2 is: {test_score.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, we get outstanding performance. We will investigate and find\n",
    "the reason for such good results with a model that is expected to fail. We\n",
    "previously mentioned that `ShuffleSplit` is an iterative cross-validation\n",
    "scheme that shuffles data and split. We will simplify this procedure with a\n",
    "single split and plot the prediction. We can use `train_test_split` for this\n",
    "purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, shuffle=True, random_state=0,\n",
    ")\n",
    "regressor.fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_test)\n",
    "# Affect the index of `y_test` to ease the plotting\n",
    "y_pred = pd.Series(y_pred, index=y_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the performance of our model on this split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "test_score = r2_score(y_test, y_pred)\n",
    "print(f\"The R2 on this single split is: {test_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain similar good results in terms of :math`R^2`. We will plot the\n",
    "training, testing and prediction samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(10, 8))\n",
    "y_train.plot(ax=ax, label=\"Training\")\n",
    "y_test.plot(ax=ax, label=\"Testing\")\n",
    "y_pred.plot(ax=ax, label=\"Prediction\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in this context, it seems that the model predictions are following the\n",
    "testing. But we can as well see that the testing samples are next to some\n",
    "training sample. And with these time-series, we see a relationship between a\n",
    "sample at the time `t` and a sample at `t+1`. In this case, we are violating\n",
    "the i.i.d. assumption. The insight to get is the following: a model can\n",
    "output of its training set at the time `t` for a testing sample at the time\n",
    "`t+1`. This prediction would be closed to the true value even if our model\n",
    "did not learn anything else than memorizing the training dataset.\n",
    "\n",
    "An easy way to verify this hypothesis is not to shuffle the data when doing\n",
    "the split. In this case, we will use the first 75% of the data to train and\n",
    "the remaining data to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, shuffle=False, random_state=0,\n",
    ")\n",
    "regressor.fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_test)\n",
    "y_pred = pd.Series(y_pred, index=y_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = r2_score(y_test, y_pred)\n",
    "print(f\"The R2 on this single split is: {test_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we see that our model is not magical anymore. Indeed, it\n",
    "performs worse than just predicting the mean of the target. We can visually\n",
    "check what we are predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(10, 8))\n",
    "y_train.plot(ax=ax, label=\"Training\")\n",
    "y_test.plot(ax=ax, label=\"Testing\")\n",
    "y_pred.plot(ax=ax, label=\"Prediction\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our model cannot predict anything because it doesn't have samples\n",
    "around the testing sample. Let's check how we could have made a proper\n",
    "cross-validation scheme to get a reasonable performance estimate.\n",
    "\n",
    "One solution would be to group the samples into time blocks, e.g. by quarter,\n",
    "and predict each group's information by using information from the other\n",
    "groups. We can use the `LeaveOneGroupOut` cross-validation for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "groups = quotes.index.to_period(\"Q\")\n",
    "cv = LeaveOneGroupOut()\n",
    "results = cross_validate(\n",
    "    regressor, X, y, cv=cv, groups=groups,\n",
    "    n_jobs=-1\n",
    ")\n",
    "test_score = results[\"test_score\"]\n",
    "print(f\"The mean R2 is: {test_score.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we see that we cannot make good predictions, which is less\n",
    "surprising than our original results.\n",
    "\n",
    "Another thing to consider is the actual application of our solution. If our\n",
    "model is aimed at forecasting (i.e., predicting future data from past data),\n",
    "we should not use training data that are ulterior to the testing data. In\n",
    "this case, we can use the `TimeSeriesSplit` cross-validation to enforce this\n",
    "behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=groups.nunique())\n",
    "results = cross_validate(\n",
    "    regressor, X, y, cv=cv, groups=groups,\n",
    "    n_jobs=-1\n",
    ")\n",
    "test_score = results[\"test_score\"]\n",
    "print(f\"The mean R2 is: {test_score.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested cross-validation\n",
    "Cross-validation is a powerful tool to evaluate the performance of a model.\n",
    "It is also used to select the best model from a pool of models. This pool of\n",
    "models can be the same family of predictor but with different parameters. In\n",
    "this case, we call this procedure **fine-tuning** of the model\n",
    "hyperparameters.\n",
    "\n",
    "We could also imagine that we would like to choose among heterogeneous models\n",
    "that will similarly use the cross-validation.\n",
    "\n",
    "In the example below, we show a minimal example of using the utility\n",
    "`GridSearchCV` to find the best parameters via cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "param_grid = {\n",
    "    \"C\": [0.1, 1, 10],\n",
    "    \"gamma\": [.01, .1],\n",
    "}\n",
    "\n",
    "model = GridSearchCV(\n",
    "    estimator=SVC(),\n",
    "    param_grid=param_grid,\n",
    "    cv=KFold(),\n",
    "    n_jobs=-1,\n",
    ")\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recall that `GridSearchCV` will train a model with some specific parameter\n",
    "on a training set and evaluate it on testing. However, this evaluation is\n",
    "done via cross-validation using the `cv` parameter. This procedure is\n",
    "repeated for all possible combinations of parameters given in `param_grid`.\n",
    "\n",
    "The attribute `best_params_` will give us the best set of parameters that\n",
    "maximize the mean score on the internal test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The best parameter found are: {model.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now show the mean score obtained using the parameter `best_score_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The mean score in CV is: {model.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, one should be extremely careful using this score. The\n",
    "misinterpretation would be the following: since the score was computed on a\n",
    "test set, it could be considered our model's generalization score.\n",
    "\n",
    "However, we should not forget that we used this score to pick-up the best\n",
    "model. It means that we used knowledge from the test set (i.e. test score) to\n",
    "decide our model's training parameter.\n",
    "\n",
    "Thus, this score is not a reasonable estimate of our generalization error.\n",
    "Indeed, we can show that it will be too optimistic in practice. The good way\n",
    "is to use a \"nested\" cross-validation. We will use an inner cross-validation\n",
    "corresponding to the previous procedure shown to optimize the\n",
    "hyper-parameters. We will also include this procedure within an outer\n",
    "cross-validation, which will be used to estimate the generalization error of\n",
    "our fine-tuned model.\n",
    "\n",
    "In this case, our inner cross-validation will always get the training set of\n",
    "the outer cross-validation, making it possible to compute the generalization\n",
    "score on a completely independent set.\n",
    "\n",
    "We will show below how we can create such nested cross-validation and obtain\n",
    "the generalization score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the inner and outer cross-validation\n",
    "inner_cv = KFold(n_splits=4, shuffle=True, random_state=0)\n",
    "outer_cv = KFold(n_splits=4, shuffle=True, random_state=0)\n",
    "\n",
    "# Inner cross-validation for parameter search\n",
    "model = GridSearchCV(\n",
    "    estimator=SVC(), param_grid=param_grid, cv=inner_cv,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Outer cross-validation to compute the generalization score\n",
    "result = cross_validate(\n",
    "    model, X, y, cv=outer_cv, n_jobs=-1,\n",
    ")\n",
    "test_score = result[\"test_score\"].mean()\n",
    "print(\n",
    "    f\"The mean score using nested cross-validation is: \"\n",
    "    f\"{test_score.mean():.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, the reported score is more trustful and should be close\n",
    "to production's expected performance.\n",
    "\n",
    "We will illustrate the difference between the nested and non-nested\n",
    "cross-validation scores to show that the latter one will be too optimistic in\n",
    "practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score_not_nested = []\n",
    "test_score_nested = []\n",
    "\n",
    "N_TRIALS = 20\n",
    "for i in range(N_TRIALS):\n",
    "    inner_cv = KFold(n_splits=4, shuffle=True, random_state=i)\n",
    "    outer_cv = KFold(n_splits=4, shuffle=True, random_state=i)\n",
    "\n",
    "    # Non_nested parameter search and scoring\n",
    "    model = GridSearchCV(\n",
    "        estimator=SVC(), param_grid=param_grid, cv=inner_cv,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    model.fit(X, y)\n",
    "    test_score_not_nested.append(model.best_score_)\n",
    "\n",
    "    # Nested CV with parameter optimization\n",
    "    result = cross_validate(\n",
    "        model, X, y, cv=outer_cv, n_jobs=-1,\n",
    "    )\n",
    "    test_score_nested.append(result[\"test_score\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"Not nested CV\": test_score_not_nested,\n",
    "        \"Nested CV\": test_score_nested,\n",
    "    }\n",
    ")\n",
    "ax = df.plot(kind=\"box\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "_ = ax.set_title(\n",
    "    \"Comparison of mean accuracy obtained on the test sets with\\n\"\n",
    "    \"and without nested cross-validation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the model's performance with the nested cross-validation is\n",
    "not as good as the non-nested cross-validation.\n",
    "\n",
    "## Take away\n",
    "In this notebook, we presented the framework used in machine-learning to\n",
    "evaluate a predictive model's performance: the cross-validation.\n",
    "\n",
    "Besides, we presented several splitting strategies that can be used in the\n",
    "general cross-validation framework. These strategies should be used wisely\n",
    "when encountering some specific patterns or types of data.\n",
    "\n",
    "Finally, we show how to perform nested cross-validation to select an optimal\n",
    "model and evaluate its generalization performance."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
