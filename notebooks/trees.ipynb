{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree in depth\n",
    "\n",
    "In this notebook, we will discuss in detail the internal algorithm used to\n",
    "build the decision tree. First, we will focus on the classification decision\n",
    "tree. Then, we will highlight the fundamental difference between the\n",
    "decision tree used for classification and regression. Finally, we will\n",
    "quickly discuss the importance of the hyper-parameters to be aware of when\n",
    "using decision trees.\n",
    "\n",
    "## Presentation of the dataset\n",
    "\n",
    "We will use the\n",
    "[Palmer penguins dataset](https://allisonhorst.github.io/palmerpenguins/).\n",
    "This dataset is comprised of penguin records and ultimately, we want to\n",
    "predict the species each penguin belongs to.\n",
    "\n",
    "Each penguin is from one of the three following species: Adelie, Gentoo, and\n",
    "Chinstrap. See the illustration below depicting the three different penguin\n",
    "species:\n",
    "\n",
    "![Image of penguins](https://github.com/allisonhorst/palmerpenguins/raw/master/man/figures/lter_penguins.png)\n",
    "\n",
    "This problem is a classification problem since the target is categorical.\n",
    "We will limit our input data to a subset of the original features\n",
    "to simplify our explanations when presenting the decision tree algorithm.\n",
    "Indeed, we will use feature based on penguins' culmen measurement. You can\n",
    "learn more about the penguins' culmen with illustration below:\n",
    "\n",
    "![Image of culmen](https://github.com/allisonhorst/palmerpenguins/raw/master/man/figures/culmen_depth.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"../datasets/penguins.csv\")\n",
    "\n",
    "# select the features of interest\n",
    "culmen_columns = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]\n",
    "target_column = \"Species\"\n",
    "\n",
    "data = data[culmen_columns + [target_column]]\n",
    "data[target_column] = data[target_column].str.split().str[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the dataset more into details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that there are 2 missing records in this dataset and for the\n",
    "sake of simplicity, we will drop the records corresponding to these 2\n",
    "samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will separate the target from the data and create a training and a\n",
    "testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = data[culmen_columns], data[target_column]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going into detail about the decision tree algorithm, we will quickly\n",
    "inspect our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "_ = sns.pairplot(data=data, hue=\"Species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can first check the feature distributions by looking at the diagonal plots\n",
    "of the pairplot. We can build the following intuitions:\n",
    "\n",
    "* The Adelie species is separable from the Gentoo and Chinstrap species using\n",
    "  the culmen length;\n",
    "* The Gentoo species is separable from the Adelie and Chinstrap species using\n",
    "  the culmen depth.\n",
    "\n",
    "## How are decision tree built?\n",
    "\n",
    "In a previous notebook, we learnt that a linear classifier will define a\n",
    "linear separation to split classes using a linear combination of the input\n",
    "features. In our 2-dimensional space, it means that a linear classifier will\n",
    "define some oblique lines that best separate our classes. We define a\n",
    "function below that, given a set of data points and a classifier, will plot\n",
    "the decision boundaries learnt by the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_decision_function(X, y, clf, ax=None):\n",
    "    \"\"\"Plot the boundary of the decision function of a classifier.\"\"\"\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # create a grid to evaluate all possible samples\n",
    "    plot_step = 0.02\n",
    "    feature_0_min, feature_0_max = (X.iloc[:, 0].min() - 1,\n",
    "                                    X.iloc[:, 0].max() + 1)\n",
    "    feature_1_min, feature_1_max = (X.iloc[:, 1].min() - 1,\n",
    "                                    X.iloc[:, 1].max() + 1)\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.arange(feature_0_min, feature_0_max, plot_step),\n",
    "        np.arange(feature_1_min, feature_1_max, plot_step)\n",
    "    )\n",
    "\n",
    "    # compute the associated prediction\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = LabelEncoder().fit_transform(Z)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # make the plot of the boundary and the data samples\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    ax.contourf(xx, yy, Z, alpha=0.4)\n",
    "    sns.scatterplot(\n",
    "        data=pd.concat([X, y], axis=1),\n",
    "        x=X.columns[0], y=X.columns[1], hue=y.name,\n",
    "        ax=ax,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, for a linear classifier, we will obtain the following decision\n",
    "boundaries. These boundaries lines indicate where the model changes its\n",
    "prediction from one class to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "linear_model = LogisticRegression()\n",
    "plot_decision_function(X_train, y_train, linear_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the lines are a combination of the input features since they are\n",
    "not perpendicular a specific axis. In addition, it seems that the linear\n",
    "model would be a good candidate model for such problem as it gives good\n",
    "accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Accuracy of the {linear_model.__class__.__name__}: \"\n",
    "    f\"{linear_model.fit(X_train, y_train).score(X_test, y_test):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike linear models, decision trees will partition the space by considering\n",
    "a single feature at a time. Let's illustrate this behaviour by having\n",
    "a decision tree that only makes a single split to partition the feature\n",
    "space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=1)\n",
    "plot_decision_function(X_train, y_train, tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partitions found by the algorithm separates the data along the axis\n",
    "\"Culmen Length\", discarding the feature \"Culmen Depth\". Thus, it highlights\n",
    "that a decision tree does not use a combination of feature when making a\n",
    "split. We can look more in depth the tree structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "_, ax = plt.subplots(figsize=(8, 6))\n",
    "_ = plot_tree(tree, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the split was done the first feature `X[0]` (i.e. \"Culmen\n",
    "Length\"). The original dataset was subdivided into 2 sets depending if the\n",
    "culmen length was inferior or superior to 43.25 mm.\n",
    "\n",
    "This partition of the dataset is the one that minimize the class diversities\n",
    "in each sub-partitions. This measure is also known as called **criterion**\n",
    "and different criterion can be used when instantiating the decision tree.\n",
    "Here, it corresponds to the Gini impurity.\n",
    "\n",
    "If we look closely at the partition, the sample inferior to 43.25 belong\n",
    "mainly to the Adelie class. Looking at the tree structure, we indeed observe\n",
    "109 Adelie samples. We also count 3 Chinstrap samples and 6 Gentoo samples.\n",
    "We can make similar interpretation for the partition defined by a threshold\n",
    "superior to 43.25 mm. In this case, the most represented class is the Gentoo\n",
    "specie.\n",
    "\n",
    "Let's see how our tree would work as a predictor. Let's start to see the\n",
    "class predicted when the culmen length is inferior to the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.predict([[40, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class predicted is the Adelie. We can now check if we pass a culmen\n",
    "length superior to the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.predict([[50, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the tree predict the Gentoo specie.\n",
    "\n",
    "Thus, we can conclude that a decision tree classifier will predict the most\n",
    "represented class within a partition.\n",
    "\n",
    "Since that during the training, we have a count of samples in each partition,\n",
    "we can also compute a probability to belong to a certain class within this\n",
    "partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.predict_proba([[50, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can manually compute the different probability directly from the tree\n",
    "structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Probabilities for the different classes:\\n\"\n",
    "    f\"Adelie: {4 / 138:.3f}\\n\"\n",
    "    f\"Chinstrap: {48 / 138:.3f}\\n\"\n",
    "    f\"Gentoo: {86 / 138:.3f}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also important to note that the culmen depth has been disregarded for\n",
    "the moment. It means that whatever the value given, it will not be used\n",
    "during the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.predict_proba([[50, 10000]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to our classification problem, the split found with a maximum\n",
    "depth of 1 is not powerful enough to separate the three species and the model\n",
    "accuracy is low when compared to the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Accuracy of the {tree.__class__.__name__}: \"\n",
    "    f\"{tree.fit(X_train, y_train).score(X_test, y_test):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, it is not a surprise. We saw earlier that a single feature will not\n",
    "be able to separate all three species. However, from the previous analysis we\n",
    "saw that by using both features we should be able to get fairly good results.\n",
    "Considering the splitting mechanism of the decision tree illustrated above,\n",
    "we should repeat the partitioning on the resulting rectangles created by the\n",
    "first split. In this regard, we expect that the two partitions at the second\n",
    "level of the tree will be using the feature \"Culmen Depth\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=2)\n",
    "plot_decision_function(X_train, y_train, tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the decision tree made two new partitions using the \"Culmen\n",
    "Depth\". Now, our tree is more powerful with similar performance to our linear\n",
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Accuracy of the {tree.__class__.__name__}: \"\n",
    "    f\"{tree.fit(X_train, y_train).score(X_test, y_test):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "At this stage, we have the intuition that a decision tree is built by\n",
    "successively partitioning the feature space, considering one feature at a\n",
    "time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We predict an Adelie penguin if the feature value is below the threshold,\n",
    "which is not surprising since this partition was almost pure. If the feature\n",
    "value is above the threshold, we predict the Gentoo penguin, the class that\n",
    "is most probable.\n",
    "\n",
    "## What about decision tree for regression?\n",
    "\n",
    "We explained the construction of the decision tree for a classification\n",
    "problem. In classification, we show that we minimized the class diversity. In\n",
    "regression, this criterion cannot be applied since `y` is continuous. To give\n",
    "some intuitions regarding the problem solved in regression, let's observe the\n",
    "characteristics of decision trees used for regression.\n",
    "\n",
    "### Decision tree: a non-parametric model\n",
    "\n",
    "We will use the same penguins dataset however, this time we will formulate a\n",
    "regression problem instead of a classification problem. We will try to infer\n",
    "the body mass of a penguin given its flipper length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../datasets/penguins.csv\")\n",
    "\n",
    "data_columns = [\"Flipper Length (mm)\"]\n",
    "target_column = \"Body Mass (g)\"\n",
    "\n",
    "data = data[data_columns + [target_column]]\n",
    "data = data.dropna()\n",
    "\n",
    "X, y = data[data_columns], data[target_column]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=data, x=\"Flipper Length (mm)\", y=\"Body Mass (g)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we deal with a regression problem because our target is a continuous\n",
    "variable ranging from 2.7 kg to 6.3 kg. From the scatter plot above, we can\n",
    "observe that we have a linear relationship between the flipper length\n",
    "and the body mass. The longer the flipper of a penguin, the heavier the\n",
    "penguin.\n",
    "\n",
    "For this problem, we would expect the simple linear model to be able to\n",
    "model this relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_model = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "We will first create a function in charge of plotting the dataset and\n",
    "all possible predictions. This function is equivalent to the earlier\n",
    "function used to plot the decision boundaries for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regression_model(X, y, model, extrapolate=False, ax=None):\n",
    "    \"\"\"Plot the dataset and the prediction of a learnt regression model.\"\"\"\n",
    "    # train our model\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # make a scatter plot of the input data and target\n",
    "    training_data = pd.concat([X, y], axis=1)\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    sns.scatterplot(\n",
    "        data=training_data, x=\"Flipper Length (mm)\", y=\"Body Mass (g)\",\n",
    "        ax=ax, color=\"black\", alpha=0.5,\n",
    "    )\n",
    "\n",
    "    # only necessary if we want to see the extrapolation of our model\n",
    "    offset = 20 if extrapolate else 0\n",
    "\n",
    "    # generate a testing set spanning between min and max of the training set\n",
    "    X_test = np.linspace(\n",
    "        X.min() - offset, X.max() + offset, num=100\n",
    "    ).reshape(-1, 1)\n",
    "\n",
    "    # predict for this testing set and plot the response\n",
    "    y_pred = model.predict(X_test)\n",
    "    ax.plot(\n",
    "        X_test, y_pred,\n",
    "        label=f\"{model.__class__.__name__} trained\", linewidth=3,\n",
    "    )\n",
    "    plt.legend()\n",
    "    # return the axes in case we want to add something to it\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "_ = plot_regression_model(X_train, y_train, linear_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the plot above, we see that a non-regularized `LinearRegression` is able\n",
    "to fit the data. A feature of this model is that all new predictions\n",
    "will be on the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "X_test_subset = X_test[:10]\n",
    "ax = plot_regression_model(X_train, y_train, linear_model)\n",
    "y_pred = linear_model.predict(X_test_subset)\n",
    "ax.plot(\n",
    "    X_test_subset, y_pred, label=\"Test predictions\",\n",
    "    color=\"tab:green\", marker=\"^\", markersize=10, linestyle=\"\",\n",
    ")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrary to linear models, decision trees are non-parametric models, so they\n",
    "do not make assumptions about the way data are distributed. This will affect\n",
    "the prediction scheme. Repeating the above experiment will highlight the\n",
    "differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree = DecisionTreeRegressor(max_depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_regression_model(X_train, y_train, tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the decision tree model does not have a priori distribution for\n",
    "the data and we do not end-up with a straight line to regress flipper length\n",
    "and body mass.\n",
    "\n",
    "Instead, we observe that the predictions of the tree are piecewise constant.\n",
    "Indeed, our feature space was split into two partitions. We can check the\n",
    "tree structure to see what was the threshold found during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The threshold for our feature (flipper length) is 206.5 mm. The predicted\n",
    "values on each side of the split are two constants: 3686.29 g and 5025.99 g.\n",
    "These values corresponds to the mean values of the training samples in each\n",
    "partition.\n",
    "\n",
    "Increasing the depth of the tree will increase the number of partition and\n",
    "thus the number of constant values that the tree is capable of predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeRegressor(max_depth=3)\n",
    "_ = plot_regression_model(X_train, y_train, tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lead us to question whether or not our decision trees are able to\n",
    "extrapolate to unseen data. We can highlight that this is possible with the\n",
    "linear model because it is a parametric model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_regression_model(X_train, y_train, linear_model, extrapolate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear model will extrapolate using the fitted model for flipper lengths\n",
    "< 175 mm and > 235 mm. Let's see the difference between the classification\n",
    "and regression trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_regression_model(X_train, y_train, linear_model, extrapolate=True)\n",
    "_ = plot_regression_model(X_train, y_train, tree, extrapolate=True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the regression tree, we see that it cannot extrapolate outside of the\n",
    "flipper length range present in the training data.\n",
    "For flipper lengths below the minimum, the mass of the penguin in the\n",
    "training data with the shortest flipper length will always be predicted.\n",
    "Similarly, for flipper lengths above the maximum, the mass of the penguin\n",
    "in the training data with the longest flipper will always predicted.\n",
    "\n",
    "## Importance of decision tree hyper-parameters on generalization\n",
    "\n",
    "This last section will illustrate the importance of some key hyper-parameters\n",
    "of the decision tree. We will illustrate it on both the classification and\n",
    "regression probelms that we previously used.\n",
    "\n",
    "### Creation of the classification and regression dataset\n",
    "\n",
    "We will first regenerate the classification and regression dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../datasets/penguins.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clf_columns = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]\n",
    "target_clf_column = \"Species\"\n",
    "\n",
    "data_clf = data[\n",
    "    data_clf_columns + [target_clf_column]\n",
    "]\n",
    "data_clf[target_clf_column] = data_clf[\n",
    "    target_clf_column].str.split().str[0]\n",
    "data_clf = data_clf.dropna()\n",
    "\n",
    "X_clf, y_clf = data_clf[data_clf_columns], data_clf[target_clf_column]\n",
    "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n",
    "    X_clf, y_clf, stratify=y_clf, random_state=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reg_columns = [\"Flipper Length (mm)\"]\n",
    "target_reg_column = \"Body Mass (g)\"\n",
    "\n",
    "data_reg = data[data_reg_columns + [target_reg_column]]\n",
    "data_reg = data_reg.dropna()\n",
    "\n",
    "X_reg, y_reg = data_reg[data_reg_columns], data_reg[target_reg_column]\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, random_state=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "sns.scatterplot(\n",
    "    data=data_clf,\n",
    "    x=\"Culmen Length (mm)\", y=\"Culmen Depth (mm)\", hue=\"Species\",\n",
    "    ax=axs[0],\n",
    ")\n",
    "axs[0].set_title(\"Classification dataset\")\n",
    "sns.scatterplot(\n",
    "    data=data_reg, x=\"Flipper Length (mm)\", y=\"Body Mass (g)\",\n",
    "    ax=axs[1],\n",
    ")\n",
    "_ = axs[1].set_title(\"Regression dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Effect of the `max_depth` parameter\n",
    "\n",
    "In decision trees, the most important parameter to get a trade-off between\n",
    "under-fitting and over-fitting is the `max_depth` parameter. Let's build\n",
    "a shallow tree and then deeper tree (for both classification and regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "max_depth = 2\n",
    "tree_clf = DecisionTreeClassifier(max_depth=max_depth)\n",
    "tree_reg = DecisionTreeRegressor(max_depth=max_depth)\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "plot_decision_function(X_train_clf, y_train_clf, tree_clf, ax=axs[0])\n",
    "plot_regression_model(X_train_reg, y_train_reg, tree_reg, ax=axs[1])\n",
    "_ = fig.suptitle(f\"Shallow tree with a max-depth of {max_depth}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 30\n",
    "tree_clf = DecisionTreeClassifier(max_depth=max_depth)\n",
    "tree_reg = DecisionTreeRegressor(max_depth=max_depth)\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "plot_decision_function(X_train_clf, y_train_clf, tree_clf, ax=axs[0])\n",
    "plot_regression_model(X_train_reg, y_train_reg, tree_reg, ax=axs[1])\n",
    "_ = fig.suptitle(f\"Deep tree with a max-depth of {max_depth}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both classification and regression setting, we can observe that\n",
    "increasing the depth will make the tree model more expressive. However, a\n",
    "tree that is too deep will overfit the training data, creating partitions\n",
    "which are only be correct for \"outliers\". The `max_depth` is one of the\n",
    "hyper-parameters that one should optimize via cross-validation and\n",
    "grid-search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\"max_depth\": np.arange(2, 10, 1)}\n",
    "tree_clf = GridSearchCV(DecisionTreeClassifier(), param_grid=param_grid)\n",
    "tree_reg = GridSearchCV(DecisionTreeRegressor(), param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "plot_decision_function(X_train_clf, y_train_clf, tree_clf, ax=axs[0])\n",
    "axs[0].set_title(\n",
    "    f\"Optimal depth found via CV: {tree_clf.best_params_['max_depth']}\"\n",
    ")\n",
    "plot_regression_model(X_train_reg, y_train_reg, tree_reg, ax=axs[1])\n",
    "_ = axs[1].set_title(\n",
    "    f\"Optimal depth found via CV: {tree_reg.best_params_['max_depth']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other parameters are used to fine tune the decision tree and have less\n",
    "impact than `max_depth`."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
